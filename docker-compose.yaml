networks:
  lakehouse-net:
    driver: bridge

volumes:
  # Storage
  minio-data:
  # Metastore
  metastore-db:
  # Analytics
  superset-db:
  # Orchestration
  airflow-db:
  airflow-logs:
  # Spark
  spark-warehouse:
  spark-checkpoints:
  # dbt
  dbt-logs:

services:
  # =========================================================================
  # OBJECT STORAGE - MinIO (S3-compatible storage for Iceberg)
  # =========================================================================
  minio:
    image: minio/minio:latest
    container_name: lakehouse-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Web Console
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse-net

  # MinIO client - creates buckets on startup
  minio-setup:
    image: minio/mc:latest
    container_name: lakehouse-minio-setup
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 minio minio123;
      mc mb --ignore-existing myminio/lakehouse;
      mc mb --ignore-existing myminio/bronze;
      mc mb --ignore-existing myminio/silver;
      mc mb --ignore-existing myminio/gold;
      mc mb --ignore-existing myminio/warehouse;
      mc anonymous set download myminio/lakehouse;
      exit 0;
      "
    networks:
      - lakehouse-net

  # =========================================================================
  # HIVE METASTORE (Metadata catalog for Iceberg tables)
  # =========================================================================
  metastore-db:
    image: postgres:15
    container_name: lakehouse-metastore-db
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
      POSTGRES_HOST_AUTH_METHOD: md5
      POSTGRES_INITDB_ARGS: "--auth-host=md5 --auth-local=md5"
    volumes:
      - metastore-db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse-net

  hive-metastore:
    image: apache/hive:3.1.3
    container_name: lakehouse-hive-metastore
    depends_on:
      metastore-db:
        condition: service_healthy
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      SERVICE_OPTS: "-Xmx2g -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://metastore-db:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive"
    entrypoint: ["/bin/bash", "/opt/hive/conf/start-hive-metastore.sh"]
    ports:
      - "9083:9083"
    volumes:
      - ./docker-init-scripts/start-hive-metastore.sh:/opt/hive/conf/start-hive-metastore.sh:ro
      - ./docker-init-scripts/hive-jars/hadoop-aws-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-aws-3.1.0.jar
      - ./docker-init-scripts/hive-jars/aws-java-sdk-bundle-1.11.375.jar:/opt/hadoop/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.375.jar
      - ./docker-init-scripts/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml
    networks:
      - lakehouse-net

  # =========================================================================
  # SPARK CLUSTER (Silver layer transformations)
  # =========================================================================
  spark-master:
    image: apache/spark:3.5.0
    container_name: lakehouse-spark-master
    user: root
    ports:
      - "7077:7077"   # Spark Master
      - "8080:8080"   # Spark Master UI
    volumes:
      - ./silver/jobs:/opt/spark/jobs
      - ./gold/jobs:/opt/spark/gold-jobs
      - ./config:/app/config
      - spark-warehouse:/opt/spark/warehouse
      - spark-checkpoints:/opt/spark/checkpoints
    networks:
      - lakehouse-net
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        pip install --quiet pyyaml pyiceberg
        /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
  spark-worker-1:
    image: apache/spark:3.5.0
    container_name: lakehouse-spark-worker-1
    user: root
    depends_on:
      - spark-master
    volumes:
      - ./silver/jobs:/opt/spark/jobs
      - spark-warehouse:/opt/spark/warehouse
    networks:
      - lakehouse-net
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        pip install --quiet pyyaml pyiceberg
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  spark-worker-2:
    image: apache/spark:3.5.0
    container_name: lakehouse-spark-worker-2
    user: root
    depends_on:
      - spark-master
    volumes:
      - ./silver/jobs:/opt/spark/jobs
      - spark-warehouse:/opt/spark/warehouse
    networks:
      - lakehouse-net
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        pip install --quiet pyyaml pyiceberg
        /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  # =========================================================================
  # TRINO (Query engine for analytics)
  # =========================================================================
  trino:
    image: trinodb/trino:latest
    container_name: lakehouse-trino
    environment:
      AWS_REGION: us-east-1
    ports:
      - "8086:8080"   # Trino UI/API
    volumes:
      - ./trino/etc:/etc/trino
    depends_on:
      - hive-metastore
      - minio
    networks:
      - lakehouse-net

  # =========================================================================
  # SUPERSET (Data visualization and BI)
  # =========================================================================
  superset-db:
    image: postgres:15
    container_name: lakehouse-superset-db
    environment:
      POSTGRES_DB: superset
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
    volumes:
      - superset-db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset -d superset"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse-net

  superset:
    image: apache/superset:latest
    container_name: lakehouse-superset
    depends_on:
      superset-db:
        condition: service_healthy
      trino:
        condition: service_started
    environment:
      SUPERSET_SECRET_KEY: 'thisISaSECRET_1234changeMeInProduction'
      DATABASE_DIALECT: postgresql
      DATABASE_USER: superset
      DATABASE_PASSWORD: superset
      DATABASE_HOST: superset-db
      DATABASE_PORT: 5432
      DATABASE_DB: superset
    ports:
      - "8088:8088"
    volumes:
      - ./superset:/app/superset_home
    command: >
      sh -c "
      superset db upgrade &&
      superset fab create-admin --username admin --firstname Admin --lastname User --email admin@superset.com --password admin || true &&
      superset init &&
      gunicorn --bind 0.0.0.0:8088 --workers 4 --timeout 120 --limit-request-line 0 --limit-request-field_size 0 'superset.app:create_app()'
      "
    networks:
      - lakehouse-net

  # =========================================================================
  # AIRFLOW (Orchestration and scheduling)
  # =========================================================================
  airflow-db:
    image: postgres:15
    container_name: lakehouse-airflow-db
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - lakehouse-net

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: lakehouse-airflow-init
    depends_on:
      airflow-db:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
    entrypoint: /bin/bash
    command:
      - -c
      - |
        airflow db migrate
        airflow users create \
          --username airflow \
          --firstname Airflow \
          --lastname Admin \
          --role Admin \
          --email admin@example.com \
          --password airflow || true
    networks:
      - lakehouse-net

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: lakehouse-airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./src:/opt/airflow/src
      - ./bronze:/opt/airflow/bronze
      - ./silver:/opt/airflow/silver
      - ./gold:/opt/airflow/gold
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8089:8080"
    command: webserver
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - lakehouse-net

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: lakehouse-airflow-scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow-logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/config:/opt/airflow/config
      - ./src:/opt/airflow/src
      - ./bronze:/opt/airflow/bronze
      - ./silver:/opt/airflow/silver
      - ./gold:/opt/airflow/gold
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - lakehouse-net

  # =========================================================================
  # PYTHON INGESTOR (Bronze layer - config-driven data extraction)
  # =========================================================================
  ingestor:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: lakehouse-ingestor
    working_dir: /app
    volumes:
      - ./src:/app/src
      - ./bronze:/app/bronze
      - ./config:/app/config
      - ./config.yaml:/app/config.yaml
      - ./config.examples:/app/config.examples
    depends_on:
      minio:
        condition: service_healthy
      hive-metastore:
        condition: service_started
    environment:
      # MinIO / S3 Configuration
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio123
      AWS_ENDPOINT_URL: http://minio:9000
      AWS_REGION: us-east-1
      # Hive Metastore
      HIVE_METASTORE_URI: thrift://hive-metastore:9083
      # Application
      PYTHONPATH: "/app/src"
    networks:
      - lakehouse-net
    # Run on-demand via Airflow, not at startup
    command: tail -f /dev/null

  # =========================================================================
  # DBT (Gold layer - config-driven business transformations)
  # =========================================================================
  dbt:
    build:
      context: ./gold
      dockerfile: Dockerfile.dbt
    container_name: lakehouse-dbt
    depends_on:
      - trino
    volumes:
      - ./gold:/usr/app
      - dbt-logs:/usr/app/logs
    environment:
      DBT_PROFILES_DIR: /usr/app
    working_dir: /usr/app
    networks:
      - lakehouse-net
    # Run on-demand via Airflow
    command: tail -f /dev/null