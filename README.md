# NYC Taxi Data Platform - End-to-End Lakehouse

**Production-ready, config-driven data lakehouse platform** implementing a complete medallion architecture (Bronze â†’ Silver â†’ Gold) for NYC Taxi data.

> ğŸ—ï¸ **Key Principle**: Engineers only update YAML files - no code changes needed to ingest or transform data.

## ğŸ¯ What This Is

A **complete open-source data platform** that demonstrates modern data engineering best practices:

- **Medallion Architecture**: Bronze (raw) â†’ Silver (clean) â†’ Gold (aggregated)
- **Config-Driven**: All transformations and orchestration defined in YAML
- **Production-Ready Orchestration**: SparkSubmitOperator with health checks, dynamic task generation, and environment parameterization
- **Cloud-Native**: Runs on Docker, production-ready for Kubernetes
- **Open Standards**: Apache Iceberg, Spark, dbt, Trino
- **Automated Quality**: Data quality framework with lineage tracking and testing coverage

## ğŸ›ï¸ Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA SOURCES                             â”‚
â”‚  NYC Taxi Data | APIs | Files | Databases                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Python Ingestors   â”‚  â† Config-driven
      â”‚  (Bronze Layer)     â”‚     (YAML only)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         BRONZE LAYER (Iceberg)             â”‚
        â”‚  â€¢ Append-only, immutable                  â”‚
        â”‚  â€¢ Raw data on MinIO/S3                    â”‚
        â”‚  â€¢ Partitioned by year/month               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         SILVER LAYER (Spark)               â”‚
        â”‚  â€¢ Typed, validated, deduped               â”‚
        â”‚  â€¢ Config-driven transformations           â”‚
        â”‚  â€¢ Data quality checks                     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          GOLD LAYER (dbt)                  â”‚
        â”‚  â€¢ Business aggregates                     â”‚
        â”‚  â€¢ Analytics-ready marts                   â”‚
        â”‚  â€¢ Modeled with dbt                        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    ANALYTICS (Trino + Superset)            â”‚
        â”‚  â€¢ Ad-hoc queries with Trino               â”‚
        â”‚  â€¢ Dashboards with Superset                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                  â–²
                  â”‚ Orchestrates
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         AIRFLOW                             â”‚
        â”‚  â€¢ Schedules pipelines                      â”‚
        â”‚  â€¢ Manages dependencies                     â”‚
        â”‚  â€¢ Retry logic & monitoring                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```text

## âœ¨ Key Features

### ğŸ”§ **Config-Driven Everything**

```yaml

# config/pipelines/lakehouse_config.yaml

bronze:
  source:
    type: http
    params:
      year: 2021
      month: 1
      
silver:
  transformations:
    filters:
      - "trip_distance > 0"
      - "fare_amount > 0"
    dedupe:
      enabled: true
      
gold:
  models:
    - name: daily_trip_stats
      aggregations:
        group_by: [year, month, location]
```text

### ğŸ¯ **Separation of Concerns**

| Layer | Responsibility | Technology |
|-------|---------------|------------|
| **Airflow** | WHEN things run | Orchestration |
| **Python** | Extract raw data | Ingestors |
| **Iceberg** | Store truth | Data lake |
| **Spark** | Clean & validate | Transformations |
| **dbt** | Define business logic | SQL models |
| **Trino** | Query analytics | SQL engine |
| **Superset** | Visualize | BI tool |

### ğŸš€ **Production Ready**

- âœ… **Idempotent**: Re-run anytime, same result
- âœ… **Replayable**: Historical data reprocessing
- âœ… **Testable**: Data quality at every layer
- âœ… **Monitored**: Airflow observability
- âœ… **Scalable**: Spark clusters for big data

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop (Windows) or Docker + Docker Compose (Linux/Mac)
- 8GB+ RAM recommended
- 20GB+ disk space

### 1. Clone and Start

```powershell

# Clone repository

git clone <repo-url>
cd nyc-taxi-data-ingestion

# Start all services

docker compose up -d

# Wait ~60 seconds for services to initialize

```

### 2. Initialize Platform

```powershell

# Run setup script (Windows)

.\scripts\setup_lakehouse.ps1

# Or Linux/Mac

chmod +x scripts/setup_lakehouse.sh
./scripts/setup_lakehouse.sh
```text

### 3. Access UIs

| Service | URL | Credentials |
|---------|-----|-------------|
| **Airflow** | http://localhost:8089 | airflow / airflow |
| **MinIO Console** | http://localhost:9001 | minio / minio123 |
| **Spark UI** | http://localhost:8080 | - |
| **Trino UI** | http://localhost:8086 | - |
| **Superset** | http://localhost:8088 | admin / admin |

### 4. Run Pipeline

**Option 1: Via Airflow UI** (Recommended)
1. Go to http://localhost:8089
2. Find `nyc_taxi_medallion_pipeline` DAG
3. Click the â–¶ï¸ Play button

**Option 2: Manual Execution**
```powershell

# Bronze layer

docker exec lakehouse-ingestor python /app/bronze/ingestors/ingest_to_iceberg.py --config /app/config/pipelines/lakehouse_config.yaml

# Silver layer

docker exec lakehouse-spark-master spark-submit /opt/spark/jobs/bronze_to_silver.py

# Gold layer

docker exec lakehouse-dbt dbt run --profiles-dir /usr/app
```text

### 5. Query Data

```sql
-- Connect to Trino at localhost:8086

-- Bronze layer (raw data)
SELECT * FROM iceberg.bronze.nyc_taxi_raw LIMIT 10;

-- Silver layer (cleaned)
SELECT * FROM iceberg.silver.nyc_taxi_clean LIMIT 10;

-- Gold layer (analytics)
SELECT * FROM iceberg.gold.daily_trip_stats LIMIT 10;
```text

## ğŸ“ Project Structure

```
nyc-taxi-data-ingestion/
â”‚
â”œâ”€â”€ config/                          # All configuration (YAML only!)

â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â””â”€â”€ lakehouse_config.yaml   # Master config for entire platform

â”‚   â””â”€â”€ sources/                     # Additional source configs

â”‚
â”œâ”€â”€ bronze/                          # Raw data ingestion

â”‚   â””â”€â”€ ingestors/
â”‚       â””â”€â”€ ingest_to_iceberg.py    # Config-driven ingestion to Iceberg

â”‚
â”œâ”€â”€ silver/                          # Data cleaning & validation

â”‚   â””â”€â”€ jobs/
â”‚       â””â”€â”€ bronze_to_silver.py     # Config-driven Spark transformations

â”‚
â”œâ”€â”€ gold/                            # Analytics models

â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ analytics/              # dbt models (SQL)

â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ airflow/                         # Orchestration

â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ nyc_taxi_medallion_dag.py  # Main pipeline DAG

â”‚   â””â”€â”€ config/
â”‚
â”œâ”€â”€ trino/                           # Query engine config

â”‚   â””â”€â”€ etc/
â”‚       â””â”€â”€ catalog/
â”‚           â””â”€â”€ iceberg.properties
â”‚
â”œâ”€â”€ spark/                           # Spark jobs & JARs

â”‚   â”œâ”€â”€ jobs/
â”‚   â””â”€â”€ jars/
â”‚
â”œâ”€â”€ scripts/                         # Setup & utilities

â”‚   â”œâ”€â”€ setup_lakehouse.ps1         # Windows setup

â”‚   â””â”€â”€ setup_lakehouse.sh          # Linux/Mac setup

â”‚
â”œâ”€â”€ docker-compose.yaml              # Full stack definition

â”œâ”€â”€ requirements.txt                 # Python dependencies

â””â”€â”€ README.md                        # This file

```text

## ğŸ›ï¸ Configuration Guide

### Master Config: `config/pipelines/lakehouse_config.yaml`

This **single file** controls the entire pipeline. No code changes needed!

#### **Bronze Layer Config**

```yaml
bronze:
  source:
    type: http  # or: s3, postgres, api

    params:
      year: 2021
      month: 1
      taxi_type: yellow
      
  target:
    database: bronze
    table: nyc_taxi_raw
    storage:
      format: parquet
      partition_by: [year, month]
```text

#### **Silver Layer Config**

```yaml
silver:
  transformations:
    # Rename columns

    rename_columns:
      tpep_pickup_datetime: pickup_datetime
      
    # Type casting

    cast_columns:
      fare_amount: decimal(10,2)
      
    # Filters

    filters:
      - "trip_distance > 0"
      - "fare_amount > 0"
      
    # Deduplication

    dedupe:
      enabled: true
      partition_by: [year, month]
      order_by: ["pickup_datetime DESC"]
      
    # Derived columns

    derived_columns:
      - name: trip_duration_minutes
        expression: "(unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) / 60"
```text

#### **Gold Layer Config**

```yaml
gold:
  models:
    - name: daily_trip_stats
      aggregations:
        group_by: [year, month, day_of_week]
        measures:
          - name: total_trips
            expression: count(*)
          - name: avg_fare
            expression: avg(fare_amount)
```

## ğŸ”„ How It Works

### The Config-Driven Flow

1. **Engineer updates YAML** (`lakehouse_config.yaml`)
   - Changes year/month to ingest
   - Adds new filters or transformations
   - Defines new Gold models

2. **Airflow triggers pipeline** (scheduled or manual)
   - No code deployment needed
   - Config is read at runtime

3. **Each layer reads config**
   - Bronze: Fetches data based on source config
   - Silver: Applies transformations from YAML
   - Gold: Generates dbt models from config

4. **Data flows through medallion**
   ```
   Source â†’ Bronze (raw) â†’ Silver (clean) â†’ Gold (aggregated) â†’ Analytics
   ```

### Airflow DAG Structure

```python

# airflow/dags/nyc_taxi_medallion_dag.py

ingest_to_bronze >> transform_to_silver >> build_gold_models >> quality_checks
```text

**Linear execution**:
1. Python ingestor writes to Bronze (Iceberg)
2. Spark job transforms Bronze â†’ Silver
3. dbt builds Gold models
4. Quality checks validate results

## ğŸ› ï¸ Common Operations

### Change Data to Ingest

```yaml

# config/pipelines/lakehouse_config.yaml

bronze:
  source:
    params:
      year: 2022        # â† Change this

      month: 6          # â† Change this

      taxi_type: green  # â† Or this (yellow, green, fhv)

```text

Then trigger the DAG in Airflow.

### Add a New Transformation

```yaml

# config/pipelines/lakehouse_config.yaml

silver:
  transformations:
    derived_columns:
      - name: is_weekend         # â† New column

        expression: "dayofweek(pickup_datetime) IN (1, 7)"
```text

Re-run the Silver layer task.

### Add a New Gold Model

```yaml

# config/pipelines/lakehouse_config.yaml

gold:
  models:
    - name: weekend_vs_weekday_stats  # â† New model

      aggregations:
        group_by: [year, month, is_weekend]
        measures:
          - name: trip_count
            expression: count(*)
```

Re-run the Gold layer task, or add a new dbt SQL file.

## ğŸ“Š Data Quality

### Bronze Layer

- Not null checks on key columns
- Positive value validation
- Schema consistency

### Silver Layer

- Range checks (e.g., passenger_count 1-10)
- Referential integrity
- Deduplication
- Type validation

### Gold Layer

- Aggregate validation
- Completeness checks
- Business logic tests (dbt tests)

## ğŸ” Querying Data

### Using Trino CLI

```bash

# Connect to Trino

docker exec -it lakehouse-trino trino

# Query any layer

SELECT * FROM iceberg.bronze.nyc_taxi_raw WHERE year = 2021 LIMIT 10;
SELECT * FROM iceberg.silver.nyc_taxi_clean WHERE trip_distance > 10;
SELECT * FROM iceberg.gold.daily_trip_stats ORDER BY total_revenue DESC;
```text

### Using Python

```python
from trino.dbapi import connect

conn = connect(
    host='localhost',
    port=8086,
    catalog='iceberg',
    schema='gold',
)

cursor = conn.cursor()
cursor.execute("SELECT * FROM daily_trip_stats LIMIT 10")
rows = cursor.fetchall()
```text

## ğŸš€ Production Deployment

### Kubernetes Deployment

This platform is designed to run on Kubernetes. Key considerations:

1. **Persistent Volumes**: MinIO data, Metastore DB
2. **Secrets Management**: Use Kubernetes secrets for credentials
3. **Resource Limits**: Set appropriate CPU/memory limits
4. **Autoscaling**: Configure HPA for Spark workers
5. **Monitoring**: Integrate with Prometheus/Grafana

### Cloud Deployment

- **S3 instead of MinIO**: Change `s3.endpoint` in configs
- **AWS Glue**: Replace Hive Metastore with Glue catalog
- **Managed Airflow**: Use AWS MWAA, GCP Composer, or Astronomer
- **Managed Spark**: Use EMR, Dataproc, or Databricks

## ğŸ§ª Testing

```powershell

# Run tests

docker exec lakehouse-ingestor pytest /app/tests/

# Test data quality

docker exec lakehouse-dbt dbt test --profiles-dir /usr/app
```text

## ğŸ“š Documentation

- [Configuration Guide](docs/CONFIGURATION.md) - Detailed config options
- [Architecture Deep Dive](docs/ARCHITECTURE.md) - System design
- [Deployment Guide](docs/DEPLOYMENT.md) - Production setup
- [Troubleshooting](docs/TROUBLESHOOTING.md) - Common issues

## ğŸ¤ Contributing

This is a demonstration project. To extend:

1. **Add new sources**: Implement in `bronze/ingestors/`
2. **Add transformations**: Update config or Spark jobs
3. **Add Gold models**: Create dbt SQL files
4. **Add quality checks**: Extend Great Expectations suite

## ğŸ“ License

MIT License - See LICENSE file

## ğŸ™ Acknowledgments

- NYC TLC for open taxi data
- Apache Iceberg, Spark, Airflow communities
- dbt Labs for dbt-core

---

**Built with â¤ï¸ for modern data engineering**

```
â”œâ”€â”€ src/                          # Source code

â”‚   â”œâ”€â”€ ingest_nyc_taxi_data.py  # Main ingestion script

â”‚   â”œâ”€â”€ ingest_zones.py          # Zones ingestion script

â”‚   â””â”€â”€ config_loader.py         # Configuration parser

â”œâ”€â”€ config.examples/             # Example configurations

â”‚   â”œâ”€â”€ batch_2021_q1.yaml       # Q1 2021 batch

â”‚   â”œâ”€â”€ batch_2021_full_year.yaml # Full year 2021

â”‚   â”œâ”€â”€ batch_2025_full_year.yaml # Full year 2025

â”‚   â”œâ”€â”€ with_zones.yaml          # Single month + zones

â”‚   â””â”€â”€ zones_only.yaml          # Zones only

â”œâ”€â”€ docs/                        # Documentation

â”‚   â”œâ”€â”€ BATCH_INGESTION.md       # Batch processing guide

â”‚   â”œâ”€â”€ CONFIGURATION.md         # Configuration reference

â”‚   â”œâ”€â”€ CONFIG_EXAMPLES.md       # Config examples

â”‚   â”œâ”€â”€ ZONES_README.md          # Zones data guide

â”‚   â””â”€â”€ QUICK_REFERENCE.md       # Command reference

â”œâ”€â”€ scripts/                     # Utility scripts

â”‚   â”œâ”€â”€ verify_zones.py          # Verify zones data

â”‚   â”œâ”€â”€ example_zones_join.py    # Example queries

â”‚   â””â”€â”€ test_config_driven.py    # Integration test

â”œâ”€â”€ docker-init-scripts/         # PostgreSQL init scripts

â”œâ”€â”€ config.yaml                  # Default configuration

â”œâ”€â”€ docker-compose.yaml          # Docker orchestration

â”œâ”€â”€ Dockerfile                   # Production image

â””â”€â”€ requirements.txt             # Python dependencies

```text

## Configuration

### Basic Configuration

```yaml

# Data source

data_source:
  year: 2021
  month: 1
  base_url: "https://d37ci6vzurychx.cloudfront.net/trip-data"
  taxi_type: yellow

# Database

database:
  connection_string: "postgresql://root:root@pgdatabase:5432/ny_taxi"
  table_name: "yellow_tripdata"

# Ingestion

ingestion:
  chunk_size: 250000
  drop_existing: false
  if_exists: "replace"

# Zones (optional)

zones:
  enabled: true
  table_name: "zones"
  create_index: true
```text

### Batch Ingestion

```yaml
data_source:
  base_url: "https://d37ci6vzurychx.cloudfront.net/trip-data"
  taxi_type: yellow

data_sources:
  - year: 2021
    month: 1
  - year: 2021
    month: 2
  - year: 2021
    month: 3
```text

See [docs/CONFIG_EXAMPLES.md](docs/CONFIG_EXAMPLES.md) for more examples.

## Usage Examples

### Ingest Single Month

```bash
docker compose run --rm ingestor
```

### Ingest Q1 2021

```bash
docker compose run --rm -e CONFIG_PATH=config.examples/batch_2021_q1.yaml ingestor
```text

### Ingest Full Year 2025

```bash
docker compose run --rm -e CONFIG_PATH=config.examples/batch_2025_full_year.yaml ingestor
```text

### Ingest Zones Only

```bash
docker compose run --rm -e CONFIG_PATH=config.examples/zones_only.yaml ingestor python src/ingest_zones.py
```text

### Verify Zones Data

```bash
docker compose run --rm ingestor python scripts/verify_zones.py
```

## Access Services

- **pgAdmin**: http://localhost:8085
  - Email: admin@admin.com
  - Password: root

- **PostgreSQL**: localhost:5432
  - User: root
  - Password: root
  - Database: ny_taxi

## Performance

- **Chunk Size**: 250,000 rows per batch (optimized for COPY)
- **Method**: PostgreSQL COPY for high-speed bulk inserts
- **Indexing**: Automatic index creation after ingestion
- **Schema Fixes**: Automatic datetime column type conversion

## Data Sources

- **Trip Data**: https://d37ci6vzurychx.cloudfront.net/trip-data/
- **Zones Data**: https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv

## Documentation

- [BATCH_INGESTION.md](docs/BATCH_INGESTION.md) - Batch processing guide
- [CONFIGURATION.md](docs/CONFIGURATION.md) - Advanced configuration
- [ZONES_README.md](docs/ZONES_README.md) - Zones reference data
- [QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md) - Command quick reference

## Requirements

- Docker & Docker Compose
- 2GB+ RAM for database
- Network access to data sources

## Development

### Local Setup

```bash

# Create virtual environment

python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies

pip install -r requirements.txt

# Run locally (update connection string in config.yaml)

python src/ingest_nyc_taxi_data.py --config config.yaml
```text

### Rebuild Image

```bash
docker compose build ingestor
```text

## Troubleshooting

### Schema Mismatch

If ingesting different years with different schemas:
```bash

# Clean database

docker compose down -v

# Run ingestion

docker compose run --rm -e CONFIG_PATH=your_config.yaml ingestor
```text

### Check Logs

```bash
docker compose logs pgdatabase
docker compose logs ingestor
```

## License

MIT
